{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几种优化器的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD  Stochastic gradient descent  随机梯度下降"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "现在的SGD一般都指mini-batch gradient descent小批量梯度下降。这是当前神经网络中大多使用的优化方法， SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。\n",
    "\n",
    "缺点：\n",
    "    1 选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了\n",
    "    2 SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点，容易产生局部最优，而不能达到全局最优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。\n",
    "\n",
    "特点\n",
    "- 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的\\mu能够进行很好的加速。由于下降方向和梯度方向一致，而使t时刻的动量变大和t时刻的变化量变大，从而达到加速的目的\n",
    "- 下降中后期时，在局部最小值来回震荡的时候，$gradient->t_0$使得更新幅度增大，跳出陷阱\n",
    "- 在梯度改变方向的时候，\\mu能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nesterov项在梯度更新时做一个校正，避免前进太快，同时提高灵敏度。 将上一节中的公式展开可得：\n",
    "\n",
    "momentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量)\n",
    "\n",
    "其实，momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特点：\n",
    "- 前期$g_t$较小的时候， regularizer较大，能够放大梯度\n",
    "- 后期$g_t$较大的时候，regularizer较小，能够约束梯度\n",
    "- 适合处理稀疏梯度\n",
    "\n",
    "\n",
    "\n",
    "缺点\n",
    "- 由公式可以看出，仍依赖于人工设置一个全局学习率\n",
    "- n设置过大的话，会使regularizer过于敏感，对梯度的调节太大\n",
    "- 中后期，分母上梯度平方的累加将会越来越大，使gradient\\to0，使得训练提前结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adadelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。\n",
    "\n",
    "特点：\n",
    "\n",
    "- 训练初中期，加速效果不错，很快\n",
    "- 训练后期，反复在局部最小值附近抖动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop可以算作Adadelta的一个特例：\n",
    "\n",
    "特点：\n",
    "\n",
    "- 其实RMSprop依然依赖于全局学习率\n",
    "- RMSprop算是Adadelta的变体，效果趋于两者之间\n",
    "- 适合非平稳目标，对RNN效果比较好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。\n",
    "\n",
    "特点：\n",
    "\n",
    "- 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点\n",
    "- 对内存需求较小\n",
    "- 对不同的参数计算不同的自适应学习率\n",
    "- 适用于大多非凸优化，适用于大数据和高维空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围\n",
    "\n",
    "Adamax使学习率的边界范围更加简单"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadam类似于带有Nesterov动量项的Adam。\n",
    "\n",
    "可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经验之谈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值\n",
    "- SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠\n",
    "- 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。\n",
    "- Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。\n",
    "- 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
