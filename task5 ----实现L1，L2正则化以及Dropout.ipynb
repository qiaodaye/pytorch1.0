{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout 原理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 在训练深度神经网络的时候总是会遇到两大问题\n",
    "-容易过拟合\n",
    "-费时\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout 可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，忽略一般的特征检测器（让一半的隐层节点值为0），可以明显减少过拟合的现象，这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout说的简单一点是：我们在前向传播的时候，让某个神经元的激活值以一定概率停止工作，这样可以使模型的泛化能力更强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x:1/(1+np.exp(-x))\n",
    "layer_size = lambda x,y:(x.shape[0],4,y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #numpy实现dropout\n",
    "\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "alpha,hidden_dim,dropout_percent,do_dropout = (0.5,4,0.2,True)\n",
    "\n",
    "synapse_0 = 2*np.random.random((3,hidden_dim)) - 1\n",
    "\n",
    "synapse_1 = 2*np.random.random((hidden_dim,1)) - 1\n",
    "\n",
    "for j in range(60000):\n",
    "\n",
    "    layer_1 = (1/(1+np.exp(-(np.dot(X,synapse_0)))))\n",
    "\n",
    "    if(do_dropout):\n",
    "\n",
    "        layer_1 *= np.random.binomial([np.ones((len(X),hidden_dim))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))\n",
    "\n",
    "    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))\n",
    "\n",
    "    layer_1_delta = layer_2_delta.dot(synapse_1.T) * (layer_1 * (1-layer_1))\n",
    "\n",
    "    synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))\n",
    "\n",
    "    synapse_0 -= (alpha * X.T.dot(layer_1_delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "N_SAMPLES = 20\n",
    "N_HIDDEN = 300\n",
    "\n",
    "# training data\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\n",
    "y = x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# test data\n",
    "test_x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1) #unsqueeze是个添加一维数据 \n",
    "test_y = test_x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "test_x, test_y = Variable(test_x, volatile=True), Variable(test_y, volatile=True)\n",
    "\n",
    "# show data\n",
    "'''\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.5, label='train')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim((-2.5, 2.5))\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "net_overfitting = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "\n",
    "net_dropped = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "\n",
    "print(net_overfitting)  # net architecture\n",
    "print(net_dropped)\n",
    "\n",
    "optimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=0.01) #optim是一个优化包\n",
    "optimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(500):\n",
    "    pred_ofit = net_overfitting(x)\n",
    "    pred_drop = net_dropped(x)\n",
    "\n",
    "    loss_ofit = loss_func(pred_ofit, y)\n",
    "    loss_drop = loss_func(pred_drop, y)\n",
    "\n",
    "    optimizer_ofit.zero_grad()\n",
    "    optimizer_drop.zero_grad()\n",
    "    loss_ofit.backward()\n",
    "    loss_drop.backward()\n",
    "    optimizer_ofit.step()\n",
    "    optimizer_drop.step()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # change to eval mode in order to fix drop out effect\n",
    "        net_overfitting.eval()\n",
    "        net_dropped.eval()  # parameters for dropout differ from train mode\n",
    "\n",
    "        # plotting\n",
    "        plt.cla()\n",
    "        test_pred_ofit = net_overfitting(test_x)\n",
    "        test_pred_drop = net_dropped(test_x)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.3, label='train')\n",
    "        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.3, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout(50%)')\n",
    "        plt.text(0, -1.2, 'overfitting loss=%.4f' % loss_func(test_pred_ofit, test_y).data[0], fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.text(0, -1.5, 'dropout loss=%.4f' % loss_func(test_pred_drop, test_y).data[0], fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));plt.pause(0.1)\n",
    "\n",
    "        # change back to train mode\n",
    "        net_overfitting.train()\n",
    "        net_dropped.train()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
